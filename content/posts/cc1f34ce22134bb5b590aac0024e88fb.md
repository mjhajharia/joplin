---
title: glossary of sorts
slug: cc1f34ce22134bb5b590aac0024e88fb
tags: []
date: 2025-07-20T12:40:31.004Z
---

## Fokker-Planck Equation

The probability density function $\rho\_t$ of $X\_t$ evolves according to the Fokker-Planck equation:
$$
\frac{\partial \rho\_t}{\partial t} = \nabla \cdot \left(\rho\_t \nabla \log \frac{\rho\_t}{\nu}\right) = \nabla \cdot (\rho\_t \nabla f) + \Delta \rho\_t.
$$

The Fokker-Planck equation is the gradient flow for minimizing the relative entropy $D\_{\mathrm{KL}}(\cdot | \nu)$ in the space of probability distributions equipped with the Wasserstein $W\_2$ metric.

## Logarithmic Sobolev Inequality (LSI)

The Logarithmic Sobolev Inequality (LSI) establishes a connection between the relative entropy and the Fisher information. For a target measure $\pi \propto e^{-V}$, the inequality is given by:

$$
D\_{\mathrm{KL}}(\rho | \pi) \leq \frac{1}{2 \lambda} I(\rho | \pi),
$$

where the relative entropy is defined as:

$$
D\_{\mathrm{KL}}(\rho | \pi) = \int\_{\mathbb{R}^d} \rho(x) \log \frac{\rho(x)}{\pi(x)} , dx,
$$

and the Fisher information is:

$$
I(\rho | \pi) = \int\_{\mathbb{R}^d} \frac{|\nabla \rho(x)|^2}{\rho(x)} , dx.
$$

Here, $\lambda > 0$ is the logarithmic Sobolev constant. LSI implies exponential convergence of the Wasserstein gradient flow of the Fokker-Planck equation.

## Poincaré Inequality (PI)

The Poincare Inequality (PI), which is weaker than LSI, bounds the variance of functions under $\pi$ :
$$
\operatorname{Var}\_N(g) \leq C\_p \mathbb{E}\_N\left|\left|\nabla\_g\right|^2\right| .
$$

## Talagrand Inequality (TI)

Talagrand's inequality refines these analyses by connecting the Wasserstein distance $\mathcal{W}\_2$ to entropy. From Otto-Villani's derivation, we have:
$$
\mathcal{W}\_2^2(\rho, \pi) \leq \frac{2}{a} \mathrm{KL}(\rho | \pi)
$$

This  bridges the gap between convergence in KL divergence and geometric convergence in Wasserstein distance. Additionally, it leads to:
$$
\frac{d}{d t} W\_2^2\left(\rho\_i, \pi\right) \leq-2 a \mathcal{W}\_2^2\left(\rho\_t, \pi\right)
$$

LSI $\implies$ PI $\implies$ TI\\

Pinsker's inequality relates the $L^1$ norm to the relative entropy:
$$
|\rho - \pi|*{L^1} \leq \sqrt{2 D*{\mathrm{KL}}(\rho | \pi)}.
$$

## Latala and Oleszkiewicz inequality

((interpolating between pi and lsi))
Let $a \in\[0,1]$ and $r \in\[1,2]$ satisfy relation $r=2 /(2-a)$. Let $\mu(d x)=$ $c\_r^n \exp \left(-\left(\left|x\_1\right|^r+\left|x\_2\right|^r+\ldots+\left|x\_n\right|^r\right)\right) d x\_1 d x\_2 \ldots d x\_n$ be a probability measure on the Euclidean space $\left(R^n,|\cdot|\right)$. We prove that there exists a universal constant $C$ such that for any smooth real function $f$ on $R^n$ and any $p \in\[1,2)$

$$
E\_\mu f^2-\left(E\_\mu|f|^p\right)^{2 / p} \leq C(2-p)^a E\_\mu|\nabla f|^2
$$

## Wasserstein metric

$P\_2(\mathbb{R}^d)$ are the probability measures on $\mathbb{R}^d$ with finite second moments. The Wasserstein distance $W\_2$ is defined as:
\begin{equation}
W\_2^2(\nu, \mu) = \inf\_{\gamma \in \Pi(\nu, \mu)} \int\_{\mathbb{R}^d \times \mathbb{R}^d} |x - y|^2 , d\gamma(x, y),
\end{equation}
where $\Pi(\nu, \mu)$ is the set of all couplings of $\nu$ and $\mu$.

## Geodesically Convex

A functional $F : P\_2(\mathbb{R}^d) \to \mathbb{R}$ is geodesically convex if:
\begin{equation}
F((1 - t)\mu\_0 + t\mu\_1) \leq (1 - t)F(\mu\_0) + tF(\mu\_1), \quad \forall t \in \[0, 1],
\end{equation}

## Itô's Lemma

For an SDE of the form:
\begin{equation}
dX\_t = b(X\_t)dt + \sigma(X\_t)dB\_t,
\end{equation}
where $b : \mathbb{R}^d \to \mathbb{R}^d$ is the drift term and $\sigma : \mathbb{R}^d \to \mathbb{R}^{d \times m}$ is the diffusion term, Itô's Lemma states:
\begin{equation}
df(X\_t) = \left( \nabla f \cdot b + \frac{1}{2}\text{Tr}(\sigma^\top \nabla^2 f \sigma) \right) dt + \nabla f \cdot \sigma , dB\_t.
\end{equation}

## Girsanov's theorem

Girsanov's theorem provides a change of measure for probability laws of SDEs. Let $X\_t$ and $\bar{X}\_t$ satisfy:
$$
\begin{aligned}
& d X\_t=b\left(X\_t\right) d t+\sigma d B\_2 \\
& \vec{d} \hat{X\_t}=\tilde{b}\left(\hat{X\_t}\right) \hat{d t}+\sigma \dot{i} B\_t .
\end{aligned}
$$

Then the Radon-Nikodym derivative of the measure $P$ induced by $X\_t$ with respect to the measure $\overline{\mathbf{P}}$ induced by $\overline{\mathrm{X}}\_t$ is:
$$
\frac{d \mathrm{P}}{d \tilde{P}}=\exp \left(\int\_0^T\left(b\left(X\_s\right)-\bar{b}\left(X\_s\right)\right)^T d B\_s-\frac{1}{2} \int\_0^T\left|b\left(X\_s\right)-\bar{b}\left(X\_s\right)\right|^2 d s\right)
$$

## Nesterov's Method

Nesterov's accelerated gradient method is described by the secondorder ODE:
$$
\ddot{x}+\gamma \dot{x}+\nabla f(x)=0
$$

## optimization in riemannian manifolds

We aim to minimize $f(x)$ on a smooth Riemannian manifold $M$.
$$
x^\*=\arg \min \_{x \in M} f(x)
$$
Gradient flow: $\dot{x}=-\nabla\_x f$.\\
Exponential convergence under strong convexity: Hess $f \succeq k l$ and gradient domination: $\left|\nabla\_x f\right|^2 \geq 2 \alpha(f(x)-\min f)$.

## Exponential Convergence (Gradient Flow)

Gradient Flow Dynamics:
$$
\frac{d}{d t} f(x)=-\left|\nabla\_x f\right|^2 .
$$
Under gradient domination:
$$
\frac{d}{d t}(f(x)-\min f) \leq-2 \alpha(f(x)-\min f)
$$
Exponential convergence:
$$
f\left(x\_t\right)-\min f \leq e^{-2 x t}\left(f\left(x\_0\right)-\min f\right) .
$$

## Wasserstein Metric

Definition: For probability measures $\rho, \nu$,

$$
W\_2^2(\rho, v)=\inf \_{X \sim \rho, Y \sim v} \mathbb{E}\left\[|X-Y|^2\right]
$$

Here the gradient of $F(\rho): \nabla\_\rho F=-\nabla \cdot(\rho \nabla \phi)$, and the geodesic is the interpolation of the optimal transport map.

## Expected Value Functional

$$
F(\rho)=\mathbb{E}*\rho\[f]=\int*{\mathbb{R}^n} \rho(x) f(x) d x
$$

Gradient domination:

$$
\mathbb{E}*\rho\left\[|\nabla f|^2\right] \geq 2 x \mathbb{E}*\rho\[f-\min f]
$$

Gradient flow:

$$
\frac{\partial \rho}{\partial t}=\nabla \cdot(\rho \nabla f)
$$

## forward backward algorithm

from wibisono 2018

1. Forward step: Gradient descent for $f$.
   $$
   x\_{k+\frac{1}{2}}=x\_k-\epsilon \nabla\_x f .
   $$
2. Backward step: Proximal gradient for $g$.
   $$
   x\_{k+1}=\arg \min *x\left{g(x)+\frac{1}{2 \epsilon}\left|x-x*{k+\frac{1}{2}}\right|^2\right} .
   $$
