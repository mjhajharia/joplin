---
title: key results of wibisono 2018
slug: 6faf31a8d88148c0bb273ed1b644fd46
tags: []
date: 2025-07-22T09:56:37.373Z
---

## Lemma 2: Contraction of ULA Bias

Under strong logconcavity, the Wasserstein distance between the biased limit of ULA and $\nu$ is bounded by terms proportional to $\sqrt{\epsilon}$, with $\epsilon$ being the discretization parameter.

## Lemma 3: Fixed Point Condition for FB Algorithm

If the covariances $\Sigma\_k$ and $\Sigma$ commute, then the FB algorithm converges to a unique fixed point in covariance, ensuring that $\Sigma\_k \rightarrow \Sigma$ as $k \rightarrow \infty$.

## Worked out Example

for the FB algorithm, consider  Example 8 from appendix G, which involves sampling from an Ornstein-Uhlenbeck (OU) process with Gaussian data. Let the target distribution be $\nu=\mathcal{N}(\mu, \Sigma)$ and initialize with $\rho\_0=\mathcal{N}\left(\mu\_0, \Sigma\_0\right)$, setting $\Sigma\_0=I$ so $\Sigma\_0$ commutes with $\Sigma$. Each distribution $\rho\_k$ remains Gaussian with mean $\mu\_k$ and covariance $\Sigma\_k$.

$\nu$ is normal, so
$$
p\_\nu(x)=\frac{1}{(2 \pi)^{d / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right)
$$
$$
-\log p\_\nu(x)=-\log \left(\frac{1}{(2 \pi)^{d / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right)\right)
$$
$$
-\log p\_\nu(x)=\frac{d}{2} \log (2 \pi)+\frac{1}{2} \log |\Sigma|+\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)
$$
ignoring constants, the negative log-density of $\nu$ is:
$$
f(x) = \frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu).
$$
$$
\nabla f(x) = \Sigma^{-1}(x - \mu).
$$

### forward step

The FB algorithm applies a gradient descent step on $f(x)=\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-$ $\mu)$,  the forward step of FB with step size $\epsilon$ is:
$$
x\_{k+\frac{1}{2}} = x\_k - \epsilon \nabla f(x\_k) = x\_k - \epsilon \Sigma^{-1}(x\_k - \mu).
$$
rewriting this we get
$$
x\_{k+\frac{1}{2}} = \mu + (I - \epsilon \Sigma^{-1})(x\_k - \mu).
$$
as $x\_k \sim \mathcal{N}(\mu\_k, \Sigma\_k)$, and forward update for the mean is:
$$
\mu\_{k+\frac{1}{2}} = \mu + (I - \epsilon \Sigma^{-1})(\mu\_k - \mu).
$$

### backward step

The backward step applies a proximal update to the covariance, requiring that the resulting $x\_{k+1} \sim \mathcal{N}\left(\mu\_{k+1}, \Sigma\_{k+1}\right)$ satisfies:
$$
x\_{k+1}=\mu\_{k+1}+\left(I-\epsilon \Sigma\_{k+1}^{-1}\right)^{-1}\left(x\_{k+\frac{1}{2}}-\mu\_{k+1}\right)
$$

### mean and covariance update

also to ensure $x\_{k+1} \sim \mathcal{N}(\mu\_{k+1}, \Sigma\_{k+1})$, the covariance consistency equation is used for the following condition on $\Sigma\_{k+1}$:
$$
\Sigma\_{k+1} (I - \epsilon \Sigma\_{k+1}^{-1})^2 = \Sigma\_k (I - \epsilon \Sigma^{-1})^2.
$$
Now, the only fixed point of the covariance update equation is $\Sigma$ itself. so as $k \to \infty$, $\Sigma\_k \to \Sigma$, guaranteeing convergence of $\rho\_k$ to $\nu$.
$$
\Sigma = \Sigma (I - \epsilon \Sigma^{-1})^2,
$$

mean update across steps:
$$
\mu\_{k+1} = \mu + (I - \epsilon \Sigma^{-1})(\mu\_k - \mu) = \mu + (I - \epsilon \Sigma^{-1})^k (\mu\_0 - \mu),
$$

which converges exponentially to $\mu$. For the covariance, the equation above shows that $\Sigma\_k \rightarrow \Sigma$ as $k \rightarrow \infty$ under the condition $\epsilon \leq \lambda\_{\min }(\Sigma)$.
